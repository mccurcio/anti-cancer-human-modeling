# Exploratory Data Analysis

There are several things to check over when one is carrying out EDA. 

First there are strictly bookkeeping items that can foul calculations or cause data type errors. This preliminary section invesstigates common dataset problems?

When trying to determine if there are any problems with your data it is best to consider at least these four points.

1. Data types: check for characters where numbers should be and vice versa. Data types maybe in binary instead of ASCII or even the language might Cyrillic or bad Unicode.
2 Numerical differences: use of the point versus the comma.
3. Missing values: Do the missing values have a pattern or not.
4. File structure: File structure issues may include obtaining a vector, matrix or even an object of the incorrect dimensions due to misreading the input. 

One of the purposes of the EDA is to learn if the data needs to pre-processed or transformed before the machine learning classification. Since the data from the proteins was already in values between [0 and 1] scaling should not be an obvious problem. However skew could be a problem that may need to be dealt with. 

A second aspect of the .............

## Formulate your questions

The purpose of the EDA is seveeral fold:

1. What is range of numbers?  
1. Do any features have skew >= 2.0? Should these features be transformed?  
1. Do our distributions violate normality?  
1. To learn if the data needs to pre-processed or transformed before machine learning classification.
1. To determine if dimensionality reduction can allow us to remove unnecessary features.

Libraries  
```{r message=FALSE, warning=FALSE}
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, cache.lazy = FALSE)

Libraries = c("readr", "knitr", "seqinr", "stringr")

for(p in Libraries){ library(p, character.only = TRUE) }
```

Import data
```{r message=FALSE, warning=FALSE}
cancer_human_comps <- read_csv("../1data/cancer_human_comps.csv",
                               col_types = cols(Class = col_factor(levels = c("0","1"))))

colnames(cancer_human_comps)[244] <- "NA" 
# NOTE: Column X244 should be marked "NA".  "NA" = Asparagine + Alanine
#View(cancer_human_comps)
```

Check Data Structure
```{r}
# str(cancer_human_comps)
```

### Dataset Problems?

When trying to determine if there are any problems with your data it is best to consider at least these four points.

1. Data types: check for characters where numbers should be and vice versa. 
2. Missing values: No missing values
3. Cheeck for binary instead of ASCII or bad Unicode instead of the expected numbers, PID={A#, C#}

### Data Types

```{r}
is.data.frame(cancer_human_comps)
class(cancer_human_comps$Class)     # Col 1
class(cancer_human_comps$PID)       # Col 2
class(cancer_human_comps$TotalAA)   # Col 3
class(cancer_human_comps$A)         # Col 4
```

Use `head` & `tail`

```{r}
head(cancer_human_comps, n=2)
```
```{r}
tail(cancer_human_comps, n=2)
```

### Check for n's and na's

```{r}
dim(cancer_human_comps)
```

- Number of Proteins Per Class: **Correct**

```{r}
class_table <- data.matrix(
        cbind(Name = c("Anti-Cancer", "Human-Ctrl"),
              Count = table(cancer_human_comps$Class)))

class_table
```

Check for ANY missing values

```{r}
sapply(cancer_human_comps, function(x) any(is.na(x)))
```
-  **No missing values found.**

## Generate Descriptive Statistics And Graphics

### Determining Correlation Using `corrplot::corrplot`

A easily understandable test, is correlation 2D-plot for investigating multicollinearity or feature reduction. It is clear that fewer attributes "means decreased computational time and complexity. Secondly, if two predictors are highly correlated, this implies that they are measuring the same underlying information. Removing one should not compromise the performance of the model and might lead to a more parsimonious and interpretable model. Third, some models can be crippled by predictors with degenerate distributions".[^8]

[^8]:"Applied Predictive Modeling", Max Kuhn and Kjell Johnson, Springer Publishing, 2018, P.43

Pearson's correlation coefficient, $~~\rho_{x,y} = \frac {E \left[(X - \mu_x)(X - \mu_y) \right]} {\sigma_x \sigma_y}$

Pearson's correlation coefficient for samples, $~~r_{xy} ~=~ \frac {\sum_{i=1}^n (x_i - \bar x)(y_i - \bar y)} { {\sqrt{\sum_{i=1}^n (x_i - \bar x)^2} } {\sqrt{\sum_{i=1}^n (y_i - \bar y)^2} }  }$

```{r fig.height=7.5, fig.width=7.5, message=FALSE, warning=FALSE}
library("corrplot")
library("RColorBrewer")

chcomp_corr_mat = cor(cancer_human_comps[,c(4:423)], 
                      method = "p") # "p": Pearson test for continous variables

corrplot(abs(chcomp_corr_mat),
         title = "Correlation (Absolute Value of R) Plot Of Amino Acid Features", 
         method = "square", 
         type = "lower",
         tl.pos = "lt",
         cl.lim = c(0, 1),
         addgrid.col = NA, 
         cl.pos = "b", # Color legend position bottom.
         tl.col = "white")
```

```{r}

if (chcomp_corr_mat > 0.75) { 
        print("At least one correlation is > 0.75")
}


which(chcomp_corr_mat == max(chcomp_corr_mat), arr.ind = TRUE)


chcomp_corr_mat["A", "A"] # Just a double check

chcomp_corr_mat["A", "K"]
chcomp_corr_mat["A", "N"]
chcomp_corr_mat["A", "S"]
```

- There is **no reason to consider multicollinearity**.

### How to reduce features given high correlation (R >= |0.75|)

If the correlation plot produced any values greater than or equal to (R >= |0.75|) then we could consider feature elimination. This interesting heuristic approach would be used for determining which feature to eliminate.[^9]

[^9]:"Applied Predictive Modeling", Max Kuhn and Kjell Johnson, Springer Publishing, 2018, P.47 (http://appliedpredictivemodeling.com/)

1. Calculate the correlation matrix of the predictors.
2. Determine the two predictors associated with the largest absolute pairwise correlation (R > |0.75|), call them predictors A and B.
3. Determine the average correlation between A and the other variables. Do the same for predictor B.
4. If A has a larger average correlation, remove it; otherwise, remove predictor B.
5. Repeat Steps 2–4 until no absolute correlations are above the threshold.

### First Moment Of A Distribution - Mean

#### Investigate Grand Means Vs Vs AA Type

- This plot shows the Grand means for all seven categories of proteins versus AA type, where n=3500.

```{r message=FALSE, warning=FALSE}
AA_ave <- colMeans(cancer_human_comps[,c(4:23)])
plot(AA_ave,
     main = "Plot of Grand Means of % Composition Vs Amino Acid Type",
     ylab = "% Composition",
     xlab = "Amino Acid",
     ylim = c(0, 0.1),
     type = "b",
     xaxt = "n")
axis(1, at = 1:20, labels = names(cancer_human_comps[,c(4:23)]))
```

#### Produce Grouped Barchart Of AA Vs. Protein Category

Produce a *Grouped Barchart* of the mean percent of each of the 20 AA from all 2 groups and then Grand mean.  

**Pseudocode**  

A. Subset the 2 protein groups, {Ctrl, Anti-Canceer} & Grand-Mean of All 2 sets  
B. Determine column means for each protein class  
C. Calculate percentage values  
D. Produce Grouped Bar Plot   

A.
```{r, cache=TRUE}
ctrl_set <- cancer_human_comps[ which(cancer_human_comps$Class == 0),]
anti_cancer_set  <- cancer_human_comps[ which(cancer_human_comps$Class == 1),]
```
B.
```{r, cache=TRUE}
ctrl_means <- apply(ctrl_set[, 4:23], 2, mean)
anti_cancer_means  <- apply(anti_cancer_set[, 4:23], 2, mean)

grand_mean <- apply(cancer_human_comps[, 4:23], 2, mean)
```
C.
```{r, cache=TRUE}
data = data.frame(ctrl_means, anti_cancer_means, grand_mean)

percent_aa = as.matrix(t(100*data))
```
D.
```{r fig.height=8, fig.width=7, message=FALSE, warning=FALSE, cache=TRUE}
barplot(percent_aa, 
        ylim = c(0, 12),
        main = "Mean A.A. % Composition Of 2 Protein Groupings",
        ylab = "% AA Composition", 
        col = colorRampPalette(brewer.pal(3,"Blues"))(3), 
        legend = T,
        beside = T)
```
Stacked bar chart of mean A.A. percent composition
```{r fig.height=8, fig.width=7, message=FALSE, warning=FALSE, cache=TRUE}
barplot(percent_aa, 
        ylim = c(0, 30),
        main = "Mean A.A. % Composition Of 2 Protein Groupings",
        ylab = "% AA Composition", 
        col = colorRampPalette(brewer.pal(3,"Blues"))(3), 
        legend = T,
        beside = F)
```


### Second Moment Of A Distribution – Variance

#### Investigating Normalized Standard Deviations {-}

Standard deviations are sensitive to scale. Since I am trying to compare the standard deviations amongst different amino acid values, we can normalize them. This normalization is more commonly called coefficient of variation (CV). 

$\sigma_{normalized} (x) = \frac {\sigma (x)} {E [|x|]} ~~~ where ~~~ \sigma(x) \equiv \sqrt{ (E[x - \mu]^2) }$  

Standard Deviation for samples;
$c.v. ~=~ \frac {1}{\bar x} \sqrt{\frac {\sum_{i=1}^N {(x_i - \bar x)^2}} {N - 1}}$

- It is interesting to see that Cysteine(C) has the largest Normalized-Standard-Deviation given its very low percent composition.
```{r}
AA_var_norm <- (apply(ordered_paa_values_only, 2, sd)) / AA_ave
plot(AA_var_norm,
     main = "Plot of Normalized-Std-Dev(CV) By AA Type",
     ylab = "Normalized-Standard-Deviation (CV)",
     xlab = "Amino Acid",
     type = "b",
     xaxt = "n")
#axis(1, at = 1:20, labels = names(ordered_paa_values_only))
```

### Third Moment Of A Distribution - Skewness

Skewness values for each A.A. by Class; $~~ \gamma_1 = E\left[ \left( \frac{X - \mu}{\sigma} \right)^3 \right] ~~ where ~~ \sigma(x) \equiv \sqrt{ (E[x - \mu]^2 )}$

Skewness for samples; $~~ Skewness = \frac {1} {N} \frac {\sum_{i=1}^N (y_i - \bar y)^3 } {s^3} ~~~~ where ~~~~ s = \sqrt{\frac {\sum_{i=1}^N {(x_i - \bar x)^2}} {N - 1}}$
```{r}
AA_skewness <- (apply(ordered_paa_values_only, 2, e1071::skewness))
plot(AA_skewness,
     main = "Plot of Skewness By Amino Acid Type",
     ylab = "Skewness",
     xlab = "Amino Acid",
     type = "b",
     xaxt = "n")
axis(1, at = 1:20, labels = names(ordered_paa_values_only))
abline(h = 2, col = "red", lwd = 3)
```
```{r}
AA_skewness
```

Cysteine: Ratio of values of Highest/Lowest value
```{r}
max_val <- max(ordered_paa_values_only$C)
min_val <- min(ordered_paa_values_only$C[ordered_paa_values_only$C > 0])

ratio_Cys <- max_val / min_val
ratio_Cys
```

Isoleucine: Ratio of values of Highest/Lowest value
```{r}
max_val <- max(ordered_paa_values_only$I)
min_val <- min(ordered_paa_values_only$I[ordered_paa_values_only$I > 0])

ratio_Iso <- max_val / min_val
ratio_Iso
```

Glutamine: Ratio of values of Highest/Lowest value
```{r}
max_val <- max(ordered_paa_values_only$Q)
min_val <- min(ordered_paa_values_only$Q[ordered_paa_values_only$Q > 0])

ratio_Glu <- max_val / min_val
ratio_Glu
```

Tyrosine: Ratio of values of Highest/Lowest value
```{r}
max_val <- max(ordered_paa_values_only$Y)
min_val <- min(ordered_paa_values_only$Y[ordered_paa_values_only$Y > 0])

ratio_Tyr <- max_val / min_val
ratio_Tyr
```

>A general rule of thumb to consider is that skewed data whose ratio of the highest value to the lowest value is greater than 20 and skew >2.0 have significant skew.
>
>Applied Predictive Modeling, P.31

Generally Skew values above 2.0 are considered high.

- As we can see the amino acids {C, I, Q, Y} are above 2.0.

| Amino acid    | Skewness | Ratio of Max : Min |
|:-------------:|:--------:|:-------------------|
| C, Cyteine    | 2.912    | 208.8              |
| I, Isoleucine | 2.279    | 75.2               |
| Q, Glutamine  | 2.580    | 75.1               |
| Y, Tyrosine   | 2.044    | 79.2               |

### Data Transformations For 4 Skewed Features

- Above we found in section 2.7.4, Skew was a found to a high degree and this suggested that data transformations should be carried out on the 4 attributes. These attributes were the amino acids {C, I, Q, Y} which had skew greater than 2.0 and a ratio of (maximum : minimum) greater than 20. 

# SEE SKEWED AND RANSFORMED DATA for continuation!